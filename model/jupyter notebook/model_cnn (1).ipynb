{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1faf150-84dc-439b-a5fd-15075b9817d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulc\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "#from datasets import load_dataset, Image, Dataset\n",
    "#from transformers import AutoFeatureExtractor, ViTFeatureExtractor,ViTForImageClassification,TrainingArguments, Trainer, BeitFeatureExtractor, TrainerCallback\n",
    "#from torchvision.transforms import (\n",
    "#    CenterCrop,\n",
    "#    Compose,\n",
    "#    Normalize,\n",
    "#    RandomHorizontalFlip,\n",
    "#    RandomResizedCrop,\n",
    "#    Resize,\n",
    "#    ToTensor)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8deca57-307f-4054-bf15-bfe5ac86ce26",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5404be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\paulc\\\\colas-deep-learning\\\\model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cba61bd-71f3-4ff3-a3ee-536263f3c23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>FISSURE</th>\n",
       "      <th>REPARATION</th>\n",
       "      <th>FISSURE LONGITUDINALE</th>\n",
       "      <th>FAÏENCAGE</th>\n",
       "      <th>MISE EN DALLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BDCAEROD0000000017183099_runway_3_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDCAEROD0000000017183055_runway_1_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BDCAEROD0000000017183118_runway_1_gridsize_512...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BDCAEROD0000000017183028_runway_1_gridsize_512...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BDCAEROD0000000017183088_runway_1_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  FISSURE  REPARATION  \\\n",
       "0  BDCAEROD0000000017183099_runway_3_gridsize_512...        0           0   \n",
       "1  BDCAEROD0000000017183055_runway_1_gridsize_512...        0           0   \n",
       "2  BDCAEROD0000000017183118_runway_1_gridsize_512...        1           0   \n",
       "3  BDCAEROD0000000017183028_runway_1_gridsize_512...        1           0   \n",
       "4  BDCAEROD0000000017183088_runway_1_gridsize_512...        0           0   \n",
       "\n",
       "   FISSURE LONGITUDINALE  FAÏENCAGE  MISE EN DALLE  \n",
       "0                      1          1              0  \n",
       "1                      1          0              0  \n",
       "2                      1          0              0  \n",
       "3                      0          0              0  \n",
       "4                      0          0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('labels_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d40426-1107-4fd6-90fa-1fc08493cd85",
   "metadata": {},
   "source": [
    "# Converting images to dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64691d35-d7f2-4745-80ee-314f5c384b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x2643766C220>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing all the paths to images in a dict\n",
    "\n",
    "path_start = os.getcwd() + \"\\\\dataset\\\\train\\\\\"\n",
    "list_path = [path_start + filename for filename in os.listdir('dataset/train') ]\n",
    "path_dict = {\"image\":list_path}\n",
    "\n",
    "# Converting the dict to a dataset object\n",
    "\n",
    "dataset = Dataset.from_dict(path_dict).cast_column(\"image\", Image())\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce455b-1ba1-4021-90b5-c0df4f13b88d",
   "metadata": {},
   "source": [
    "### Label management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f197c6e-358b-477a-b39e-50658fb8c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0, 0, 1, 1, 0]\n",
       "1      [0, 0, 1, 0, 0]\n",
       "2      [1, 0, 1, 0, 0]\n",
       "3      [1, 0, 0, 0, 0]\n",
       "4      [0, 0, 0, 0, 0]\n",
       "            ...       \n",
       "825    [0, 0, 0, 0, 0]\n",
       "826    [1, 1, 1, 0, 1]\n",
       "827    [0, 1, 0, 1, 0]\n",
       "828    [0, 1, 1, 1, 0]\n",
       "829    [0, 0, 0, 1, 0]\n",
       "Name: label, Length: 830, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data.apply(lambda x: [x.FISSURE, x.REPARATION, x['FISSURE LONGITUDINALE'], x.FAÏENCAGE, x['MISE EN DALLE']] , axis=1)\n",
    "data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6b5283-21fe-4e73-abce-680dfa6e9a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BDCAEROD0000000017183099_runway_3_gridsize_512_idx_7_idy_0.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filename[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7aedf-3fd6-4d27-acac-bcfe1c87e61e",
   "metadata": {},
   "source": [
    "### Creating the column of labels to be added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617068bb-bed2-4ac3-ae1a-d2931298d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [1.0, 0.0, 1.0, 1.0, 1.0],\n",
       " [1.0, 0.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_of_labels = []\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    filename = list_path[i][65:]\n",
    "    row_dataset = data[data.filename == filename]\n",
    "    list_label = list(row_dataset['label'])[0]\n",
    "    list_label = np.array(list_label, dtype = np.float32).tolist()\n",
    "    column_of_labels.append(list_label)\n",
    "    \n",
    "column_of_labels[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4276137f-f0fe-43e4-aa9c-ee2d8ae5cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(name=\"label\", column=column_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1654e75f-4809-4a61-90ee-38b46ae4732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x26437720430>,\n",
       " 'label': [0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bea6189-6764-4a2c-aba8-450024b6bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = ['FISSURE','REPARATION','FISSURE LONGITUDINALE','FAÏENCAGE','MISE EN DALLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f226e7c8-e4b1-4b23-a8ce-80e37c1f0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k:l for k, l in enumerate(ALL_LABELS)}\n",
    "label2id = {l:k for k, l in enumerate(ALL_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c5487d-01ab-48e4-851b-870737479733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'FISSURE',\n",
       " 1: 'REPARATION',\n",
       " 2: 'FISSURE LONGITUDINALE',\n",
       " 3: 'FAÏENCAGE',\n",
       " 4: 'MISE EN DALLE'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90b5ca5-5982-4e74-868f-25247d2c1c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FISSURE': 0,\n",
       " 'REPARATION': 1,\n",
       " 'FISSURE LONGITUDINALE': 2,\n",
       " 'FAÏENCAGE': 3,\n",
       " 'MISE EN DALLE': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5a048-1412-4a4c-b9ff-90ae95b12d73",
   "metadata": {},
   "source": [
    "# Extracting pixel data from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b77b53c-85ce-40a9-8a0f-da2420159c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/vit-base-patch16-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 4 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042b660d-f3aa-4bf8-8858-33e442152c35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViTFeatureExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27956\\1646090109.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_extractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mViTFeatureExtractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ViTFeatureExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "feature_extractor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ab0f9-0b85-4966-bbe0-84fd5a66a177",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e662e3-c9a0-4ef6-864b-2935ad42f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364199a7-3ffb-450d-a3a2-080b0ec2c0fb",
   "metadata": {},
   "source": [
    "### Defining data augmentation fonctions and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7136905-8302-4f44-8b22-601c654ba793",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e4370f6-ab3e-4058-9cfb-3ff3a1ba6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "378f8264-25b4-459c-8903-9482fc955df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d43a8-a28d-49df-9f77-a3fa4b7bcef9",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207b3769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561591f9-0356-4375-8130-8d59ac3da651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 5,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint,\n",
    "    use_auth_token='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'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adccaa7c-9fed-4e95-abb6-749abb367e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_checkpoint)\n",
    "feature_extractor.save_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152b12d2-b95b-40a3-bc43-f72db369b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f46a84-5bcf-43c6-bc03-656a6c7c4380",
   "metadata": {},
   "source": [
    "# Using default trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9a0a2-2e0f-4dfc-a73c-7caee289774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=7,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=collate_fn,\n",
    "    #hub_token = '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'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d3c92-eb07-4d12-a038-d35d06e8a81d",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d5e3b-54c4-4dc9-aa5b-fd73943a21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "#trainer.save_model()\n",
    "#trainer.log_metrics(\"train\", train_results.metrics)\n",
    "#trainer.save_metrics(\"train\", train_results.metrics)\n",
    "#trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd78e24-38e7-454e-ae0e-e95e7ac5c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f38f705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14732231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc3d47b-27d0-4246-8efe-2d4b3be9e708",
   "metadata": {},
   "source": [
    "# Using custom trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890ab01-b1ce-4f38-a98d-680545bccbca",
   "metadata": {},
   "source": [
    "### Defining precisely the loss we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd85954-2f38-4ea3-b025-b1d1f5f64bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SCORE_INDICES = range(0, 5)\n",
    "\n",
    "def get_preds_from_logits(logits):\n",
    "    ret = np.zeros(logits.shape)\n",
    "    \n",
    "    # We fill 1 to every class whose score is higher than some threshold\n",
    "    # In this example, we choose that threshold = 0.0\n",
    "    ret[:, GLOBAL_SCORE_INDICES] = np.array(logits[:, GLOBAL_SCORE_INDICES] >= 0.0).astype(int)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a454e339-49f4-4eb2-bcbf-7d8c1820aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "    \n",
    "    # Deduce predictions from logits\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "\n",
    "    # The global f1_metrics\n",
    "    final_metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n",
    "    final_metrics[\"f1_macro\"] = f1_score(labels, predictions, average=\"macro\")\n",
    "    final_metrics[\"f1_weight\"] = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification report for global scores: \")\n",
    "    print(classification_report(labels[:, GLOBAL_SCORE_INDICES], predictions[:, GLOBAL_SCORE_INDICES], zero_division=0))\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b7ce5-c4c3-42bd-bf14-be5fb2a5c5a1",
   "metadata": {},
   "source": [
    "### Adapting the Huggingface trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90ebd8b7-5c8e-44ac-89b0-7d2e4a4028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits[:, GLOBAL_SCORE_INDICES], labels[:, GLOBAL_SCORE_INDICES])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2c09705-13ec-4276-ab1b-db9b831f8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b713ce-f160-41b1-864f-82e7ed21d010",
   "metadata": {},
   "source": [
    "### Running trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9023458-6cb5-4148-b7c8-85087c4b45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f1_macro\"\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrinterCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48ac5a6a-0a93-44f1-85fe-a2678e78424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 664\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 27:01, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>0.311613</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.344955</td>\n",
       "      <td>0.487651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.312056</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.363090</td>\n",
       "      <td>0.486611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.655602</td>\n",
       "      <td>0.442901</td>\n",
       "      <td>0.600154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.262213</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>0.644665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 166\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.9879518072289156: \n",
      "Classification report for global scores: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73        53\n",
      "           1       0.00      0.00      0.00        22\n",
      "           2       0.68      0.48      0.56        44\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       1.00      0.28      0.44        25\n",
      "\n",
      "   micro avg       0.76      0.42      0.54       152\n",
      "   macro avg       0.49      0.29      0.34       152\n",
      "weighted avg       0.63      0.42      0.49       152\n",
      " samples avg       0.21      0.17      0.18       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to google/vit-base-patch16-224\\checkpoint-41\n",
      "Configuration saved in google/vit-base-patch16-224\\checkpoint-41\\config.json\n",
      "Model weights saved in google/vit-base-patch16-224\\checkpoint-41\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 166\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.9879518072289155: \n",
      "Classification report for global scores: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.47      0.63        53\n",
      "           1       0.00      0.00      0.00        22\n",
      "           2       0.95      0.41      0.57        44\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       1.00      0.44      0.61        25\n",
      "\n",
      "   micro avg       0.96      0.36      0.52       152\n",
      "   macro avg       0.58      0.26      0.36       152\n",
      "weighted avg       0.77      0.36      0.49       152\n",
      " samples avg       0.16      0.13      0.14       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to google/vit-base-patch16-224\\checkpoint-82\n",
      "Configuration saved in google/vit-base-patch16-224\\checkpoint-82\\config.json\n",
      "Model weights saved in google/vit-base-patch16-224\\checkpoint-82\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 166\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2.9879518072289155: \n",
      "Classification report for global scores: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.70      0.79        53\n",
      "           1       0.00      0.00      0.00        22\n",
      "           2       0.85      0.64      0.73        44\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.93      0.56      0.70        25\n",
      "\n",
      "   micro avg       0.89      0.52      0.66       152\n",
      "   macro avg       0.54      0.38      0.44       152\n",
      "weighted avg       0.71      0.52      0.60       152\n",
      " samples avg       0.24      0.20      0.21       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to google/vit-base-patch16-224\\checkpoint-123\n",
      "Configuration saved in google/vit-base-patch16-224\\checkpoint-123\\config.json\n",
      "Model weights saved in google/vit-base-patch16-224\\checkpoint-123\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 166\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3.9879518072289155: \n",
      "Classification report for global scores: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82        53\n",
      "           1       1.00      0.09      0.17        22\n",
      "           2       0.85      0.66      0.74        44\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.94      0.60      0.73        25\n",
      "\n",
      "   micro avg       0.91      0.55      0.69       152\n",
      "   macro avg       0.75      0.41      0.49       152\n",
      "weighted avg       0.88      0.55      0.64       152\n",
      " samples avg       0.25      0.22      0.23       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to google/vit-base-patch16-224\\checkpoint-164\n",
      "Configuration saved in google/vit-base-patch16-224\\checkpoint-164\\config.json\n",
      "Model weights saved in google/vit-base-patch16-224\\checkpoint-164\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from google/vit-base-patch16-224\\checkpoint-164 (score: 0.49183360568097).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=164, training_loss=0.3026995945994447, metrics={'train_runtime': 1628.8208, 'train_samples_per_second': 1.631, 'train_steps_per_second': 0.101, 'total_flos': 2.0520430589494886e+17, 'train_loss': 0.3026995945994447, 'epoch': 3.99})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228b858-e9b6-45f1-9dd4-0d6d01080e07",
   "metadata": {},
   "source": [
    "# Trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513b5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e876ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/paulc/colas-deep-learning'+'/model/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392688d-18a3-4bf2-8dbe-4972feaed953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the image\n",
    "\n",
    "image_test = test_ds[6]['image']\n",
    "encoding = feature_extractor(image_test.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "\n",
    "# Call the model to get predictions\n",
    "\n",
    "outputs = model(**encoding)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Decode the result\n",
    "\n",
    "preds = get_preds_from_logits(logits)\n",
    "decoded_preds = [[id2label[i] for i, l in enumerate(row) if l == 1] for row in preds]\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e5642-bc4f-476d-99bb-b2b4cc192c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d7bee-bf3f-4dac-bc04-24d7ffc998b4",
   "metadata": {},
   "source": [
    "# Defining submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea220d-27be-405c-96bb-e8603213ccce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

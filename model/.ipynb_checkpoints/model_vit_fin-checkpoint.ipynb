{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1faf150-84dc-439b-a5fd-15075b9817d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "from datasets import load_dataset, Image, Dataset, concatenate_datasets\n",
    "from transformers import AutoFeatureExtractor, ViTFeatureExtractor,ViTForImageClassification,TrainingArguments, Trainer, BeitFeatureExtractor, TrainerCallback\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    RandomVerticalFlip,\n",
    "    Resize,\n",
    "    ToTensor)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8deca57-307f-4054-bf15-bfe5ac86ce26",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cba61bd-71f3-4ff3-a3ee-536263f3c23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>FISSURE</th>\n",
       "      <th>REPARATION</th>\n",
       "      <th>FISSURE LONGITUDINALE</th>\n",
       "      <th>FAÏENCAGE</th>\n",
       "      <th>MISE EN DALLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BDCAEROD0000000017183099_runway_3_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDCAEROD0000000017183055_runway_1_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BDCAEROD0000000017183118_runway_1_gridsize_512...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BDCAEROD0000000017183028_runway_1_gridsize_512...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BDCAEROD0000000017183088_runway_1_gridsize_512...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  FISSURE  REPARATION  \\\n",
       "0  BDCAEROD0000000017183099_runway_3_gridsize_512...        0           0   \n",
       "1  BDCAEROD0000000017183055_runway_1_gridsize_512...        0           0   \n",
       "2  BDCAEROD0000000017183118_runway_1_gridsize_512...        1           0   \n",
       "3  BDCAEROD0000000017183028_runway_1_gridsize_512...        1           0   \n",
       "4  BDCAEROD0000000017183088_runway_1_gridsize_512...        0           0   \n",
       "\n",
       "   FISSURE LONGITUDINALE  FAÏENCAGE  MISE EN DALLE  \n",
       "0                      1          1              0  \n",
       "1                      1          0              0  \n",
       "2                      1          0              0  \n",
       "3                      0          0              0  \n",
       "4                      0          0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('labels_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d40426-1107-4fd6-90fa-1fc08493cd85",
   "metadata": {},
   "source": [
    "# Converting images to dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64691d35-d7f2-4745-80ee-314f5c384b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x23AB7A9C130>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing all the paths to images in a dict\n",
    "\n",
    "path_start = os.getcwd() + \"\\\\dataset\\\\train\\\\\"\n",
    "list_path = [path_start + filename for filename in os.listdir('dataset/train') ]\n",
    "path_dict = {\"image\":list_path}\n",
    "\n",
    "# Converting the dict to a dataset object\n",
    "\n",
    "dataset = Dataset.from_dict(path_dict).cast_column(\"image\", Image())\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce455b-1ba1-4021-90b5-c0df4f13b88d",
   "metadata": {},
   "source": [
    "### Label management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f197c6e-358b-477a-b39e-50658fb8c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0, 0, 1, 1, 0]\n",
       "1      [0, 0, 1, 0, 0]\n",
       "2      [1, 0, 1, 0, 0]\n",
       "3      [1, 0, 0, 0, 0]\n",
       "4      [0, 0, 0, 0, 0]\n",
       "            ...       \n",
       "825    [0, 0, 0, 0, 0]\n",
       "826    [1, 1, 1, 0, 1]\n",
       "827    [0, 1, 0, 1, 0]\n",
       "828    [0, 1, 1, 1, 0]\n",
       "829    [0, 0, 0, 1, 0]\n",
       "Name: label, Length: 830, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data.apply(lambda x: [x.FISSURE, x.REPARATION, x['FISSURE LONGITUDINALE'], x.FAÏENCAGE, x['MISE EN DALLE']] , axis=1)\n",
    "data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6b5283-21fe-4e73-abce-680dfa6e9a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BDCAEROD0000000017183099_runway_3_gridsize_512_idx_7_idy_0.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filename[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7aedf-3fd6-4d27-acac-bcfe1c87e61e",
   "metadata": {},
   "source": [
    "### Creating the column of labels to be added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617068bb-bed2-4ac3-ae1a-d2931298d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [1.0, 0.0, 1.0, 1.0, 1.0],\n",
       " [1.0, 0.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_of_labels = []\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    filename = list_path[i][65:]\n",
    "    row_dataset = data[data.filename == filename]\n",
    "    list_label = list(row_dataset['label'])[0]\n",
    "    list_label = np.array(list_label, dtype = np.float32).tolist()\n",
    "    column_of_labels.append(list_label)\n",
    "    \n",
    "column_of_labels[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4276137f-f0fe-43e4-aa9c-ee2d8ae5cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(name=\"label\", column=column_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1654e75f-4809-4a61-90ee-38b46ae4732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x23AB7B3D940>,\n",
       " 'label': [0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bea6189-6764-4a2c-aba8-450024b6bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = ['FISSURE','REPARATION','FISSURE LONGITUDINALE','FAÏENCAGE','MISE EN DALLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f226e7c8-e4b1-4b23-a8ce-80e37c1f0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k:l for k, l in enumerate(ALL_LABELS)}\n",
    "label2id = {l:k for k, l in enumerate(ALL_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c5487d-01ab-48e4-851b-870737479733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'FISSURE',\n",
       " 1: 'REPARATION',\n",
       " 2: 'FISSURE LONGITUDINALE',\n",
       " 3: 'FAÏENCAGE',\n",
       " 4: 'MISE EN DALLE'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90b5ca5-5982-4e74-868f-25247d2c1c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FISSURE': 0,\n",
       " 'REPARATION': 1,\n",
       " 'FISSURE LONGITUDINALE': 2,\n",
       " 'FAÏENCAGE': 3,\n",
       " 'MISE EN DALLE': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5a048-1412-4a4c-b9ff-90ae95b12d73",
   "metadata": {},
   "source": [
    "# Extracting pixel data from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b77b53c-85ce-40a9-8a0f-da2420159c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/vit-base-patch16-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 4 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042b660d-f3aa-4bf8-8858-33e442152c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"size\": 224\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52068c0-ad4c-45fd-97ce-2437c2a2ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635b6a5ec55140dc84c688ba9fac8c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating copies of original dataset for data augmentation\n",
    "\n",
    "dataset2 = dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364199a7-3ffb-450d-a3a2-080b0ec2c0fb",
   "metadata": {},
   "source": [
    "### Defining data augmentation fonctions and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7136905-8302-4f44-8b22-601c654ba793",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling function for normalizing image size\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "### Creating pipeline objects for image preparation\n",
    "### One different pipeline for each copy of the dataset (so that we don't have duplicate images)\n",
    "\n",
    "dataset_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset2_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "### Creating transformation functions based on pipelines\n",
    "\n",
    "def preprocess_dataset(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [dataset_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_dataset2(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [dataset2_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "### Applying transformation functions\n",
    "\n",
    "dataset.set_transform(preprocess_dataset)\n",
    "dataset2.set_transform(preprocess_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c505d-466e-420f-97ee-240d274314c1",
   "metadata": {},
   "source": [
    "### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a83366-e487-4d1c-bc4e-0d1a5221ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset\n",
    "val_ds = dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d43a8-a28d-49df-9f77-a3fa4b7bcef9",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561591f9-0356-4375-8130-8d59ac3da651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 5,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint,\n",
    "    use_auth_token='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'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adccaa7c-9fed-4e95-abb6-749abb367e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_checkpoint)\n",
    "feature_extractor.save_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152b12d2-b95b-40a3-bc43-f72db369b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f46a84-5bcf-43c6-bc03-656a6c7c4380",
   "metadata": {},
   "source": [
    "# Using default trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9a0a2-2e0f-4dfc-a73c-7caee289774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=7,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=collate_fn,\n",
    "    #hub_token = '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'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d3c92-eb07-4d12-a038-d35d06e8a81d",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d5e3b-54c4-4dc9-aa5b-fd73943a21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "#trainer.save_model()\n",
    "#trainer.log_metrics(\"train\", train_results.metrics)\n",
    "#trainer.save_metrics(\"train\", train_results.metrics)\n",
    "#trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd78e24-38e7-454e-ae0e-e95e7ac5c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3d47b-27d0-4246-8efe-2d4b3be9e708",
   "metadata": {},
   "source": [
    "# Using custom trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890ab01-b1ce-4f38-a98d-680545bccbca",
   "metadata": {},
   "source": [
    "### Defining precisely the loss we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd85954-2f38-4ea3-b025-b1d1f5f64bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SCORE_INDICES = range(0, 5)\n",
    "\n",
    "def get_preds_from_logits(logits):\n",
    "    ret = np.zeros(logits.shape)\n",
    "    \n",
    "    # We fill 1 to every class whose score is higher than some threshold\n",
    "    # In this example, we choose that threshold = 0.0\n",
    "    ret[:, GLOBAL_SCORE_INDICES] = np.array(logits[:, GLOBAL_SCORE_INDICES] >= 0.0).astype(int)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a454e339-49f4-4eb2-bcbf-7d8c1820aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "    \n",
    "    # Deduce predictions from logits\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "\n",
    "    # The global f1_metrics\n",
    "    final_metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n",
    "    final_metrics[\"f1_macro\"] = f1_score(labels, predictions, average=\"macro\")\n",
    "    final_metrics[\"f1_weight\"] = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification report for global scores: \")\n",
    "    print(classification_report(labels[:, GLOBAL_SCORE_INDICES], predictions[:, GLOBAL_SCORE_INDICES], zero_division=0))\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b7ce5-c4c3-42bd-bf14-be5fb2a5c5a1",
   "metadata": {},
   "source": [
    "### Adapting the Huggingface trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90ebd8b7-5c8e-44ac-89b0-7d2e4a4028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits[:, GLOBAL_SCORE_INDICES], labels[:, GLOBAL_SCORE_INDICES])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2c09705-13ec-4276-ab1b-db9b831f8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b713ce-f160-41b1-864f-82e7ed21d010",
   "metadata": {},
   "source": [
    "### Running trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9023458-6cb5-4148-b7c8-85087c4b45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f1_macro\"\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrinterCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac5a6a-0a93-44f1-85fe-a2678e78424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 830\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24/312 03:03 < 40:06, 0.12 it/s, Epoch 0.44/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228b858-e9b6-45f1-9dd4-0d6d01080e07",
   "metadata": {},
   "source": [
    "# Trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392688d-18a3-4bf2-8dbe-4972feaed953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the image\n",
    "\n",
    "image_test = test_ds[6]['image']\n",
    "encoding = feature_extractor(image_test.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "\n",
    "# Call the model to get predictions\n",
    "\n",
    "outputs = model(**encoding)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Decode the result\n",
    "\n",
    "preds = get_preds_from_logits(logits)\n",
    "decoded_preds = [[id2label[i] for i, l in enumerate(row) if l == 1] for row in preds]\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e5642-bc4f-476d-99bb-b2b4cc192c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d7bee-bf3f-4dac-bc04-24d7ffc998b4",
   "metadata": {},
   "source": [
    "# Defining submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea220d-27be-405c-96bb-e8603213ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating test dataset\n",
    "\n",
    "path_start_test = os.getcwd() + \"\\\\dataset\\\\test\\\\\"\n",
    "list_path_test = [path_start_test + filename for filename in os.listdir('dataset/test') ]\n",
    "path_dict_test = {\"image\":list_path_test}\n",
    "\n",
    "# Converting the dict to a dataset object\n",
    "\n",
    "dataset_test = Dataset.from_dict(path_dict_test).cast_column(\"image\", Image())\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735ab18-d9b5-479a-bd71-9a26f9e5c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test = pd.read_csv('template_test.csv')\n",
    "template_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee0284-f800-4f6c-8f9a-2f1bbd6ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test['filepath'] = template_test.apply(lambda x: os.getcwd() + \"\\\\dataset\\\\test\\\\\" + x.filename,axis=1)\n",
    "template_test.filepath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10113ace-9243-4e9b-a65a-0e9c79e1621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pred(filepath):\n",
    "    image_to_decode = PIL.Image.open(filepath)\n",
    "    encoding = feature_extractor(image_to_decode.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    preds = get_preds_from_logits(logits)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702e542-2d5c-4d85-837b-05999c37cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test['predictions'] = template_test.apply(lambda x: return_pred(x.filepath),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891eca6-3c7c-474c-a744-a0e94c6f1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec8da6-1188-4496-992a-4e9e522d8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test[\"FISSURE\"] = template_test.apply(lambda x: int(x.predictions.tolist()[0][0]) ,axis=1)\n",
    "template_test[\"REPARATION\"] = template_test.apply(lambda x: int(x.predictions.tolist()[0][1]),axis=1)\n",
    "template_test[\"FISSURE LONGITUDINALE\"] = template_test.apply(lambda x: int(x.predictions.tolist()[0][2]),axis=1)\n",
    "template_test[\"FAÏENCAGE\"] = template_test.apply(lambda x: int(x.predictions.tolist()[0][3]),axis=1)\n",
    "template_test[\"MISE EN DALLE\"] = template_test.apply(lambda x: int(x.predictions.tolist()[0][4]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef68cd-8fc9-4032-8640-74eac20b23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14944c-b855-4fea-999d-3e6815135284",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = template_test.drop(columns=['filepath','predictions'], axis=1)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a0006-5b1c-44d3-86d4-c68aeaed4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf87641-2361-471d-a6d9-b7140d6ec538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
